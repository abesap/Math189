
@article{quintana_futures_nodate,
	title = {Futures {Markets}, {Bayesian} {Forecasting} and {Risk} {Modeling}},
	abstract = {Applications of the Bayesian approach to risk modeling regarding speculating trading strategies in Futures Markets is discussed in the context of the corresponding concepts of betting and investing, prices and expectations, and coherence and arbitrage-free pricing in the ﬁelds of Bayesian methodology and Finance.},
	language = {en},
	author = {Quintana, Jose Mario and Carvalho, Carlos M and Scott, James and Costigliola, Thomas},
	pages = {17},
	file = {Quintana et al. - Futures Markets, Bayesian Forecasting and Risk Mod.pdf:/Users/abel/Zotero/storage/YFYQ9N8J/Quintana et al. - Futures Markets, Bayesian Forecasting and Risk Mod.pdf:application/pdf}
}

@article{gerlein_evaluating_2016,
	title = {Evaluating machine learning classification for financial trading: {An} empirical approach},
	volume = {54},
	issn = {09574174},
	shorttitle = {Evaluating machine learning classification for financial trading},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417416000282},
	doi = {10.1016/j.eswa.2016.01.018},
	abstract = {Technical and quantitative analysis in financial trading use mathematical and statistical tools to help investors decide on the optimum moment to initiate and close orders. While these traditional approaches have served their purpose to some extent, new techniques arising from the field of computational intelligence such as machine learning and data mining have emerged to analyse financial information. While the main financial engineering research has focused on complex computational models such as Neural Networks and Support Vector Machines, there are also simpler models that have demonstrated their usefulness in applications other than financial trading, and are worth considering to determine their advantages and inherent limitations when used as trading analysis tools. This paper analyses the role of simple machine learning models to achieve profitable trading through a series of trading simulations in the FOREX market. It assesses the performance of the models and how particular setups of the models produce systematic and consistent predictions for profitable trading. Due to the inherent complexities of financial time series the role of attribute selection, periodic retraining and training set size are discussed in order to obtain a combination of those parameters not only capable of generating positive cumulative returns for each one of the machine learning models but also to demonstrate how simple algorithms traditionally precluded from financial forecasting for trading applications presents similar performances as their more complex counterparts. The paper discusses how a combination of attributes in addition to technical indicators that has been used as inputs of the machine learning-based predictors such as price related features, seasonality features and lagged values used in classical time series analysis are used to enhance the classification capabilities that impacts directly into the final profitability.},
	language = {en},
	urldate = {2019-02-28},
	journal = {Expert Systems with Applications},
	author = {Gerlein, Eduardo A. and McGinnity, Martin and Belatreche, Ammar and Coleman, Sonya},
	month = jul,
	year = {2016},
	pages = {193--207},
	file = {Gerlein et al. - 2016 - Evaluating machine learning classification for fin.pdf:/Users/abel/Zotero/storage/IRK6S8PB/Gerlein et al. - 2016 - Evaluating machine learning classification for fin.pdf:application/pdf}
}

@article{lim_naive_nodate,
	title = {Naïve {Bayes} {Classifier} {And} {Profitability} of {Options} {Gamma} {Trading}},
	abstract = {At any given time during the lifetime of financial options, one can set up a variety of delta-neutral positions. In essence, delta-neutral volatility strategies attempt to make profit from correct prediction about the two components of options price: the mean reverting properties of implied volatility and the change of intrinsic value due to underlying price change. In this project, we focus on the latter, the intrinsic value, so we will assume constant volatility across the project. Delta neutral portfolio’s sensitivity with respect to underlying is measured by the Greek letter delta and gamma, the first and second derivative of options value with respect to underlying price, respectively. Volatility traders who take long gamma position bet that market would move fast enough to cover the cost of holding their delta neutral position. If one expect that market is going to stay constant for a certain amount time, then one might want to take short gamma position and collect the decreasing time value of options as profit. Thus, in delta-neutral volatility trading, it is crucial to predict whether or not the underlying market is about to move significantly anytime soon. We will test the profitability of short gamma strategy, so we want to answer the question: is it safe to short on options to collect time value now, or is the market swing imminent so short position on options will result in incurring loss? In this project, we apply Naïve Bayes classifier to KOSPI 200 index options data to classify each day’s underlying price data into success or failure based on profitability of short gamma strategy.},
	language = {en},
	author = {Lim, Hyung Sup},
	pages = {5},
	file = {Lim - Naïve Bayes Classifier And Profitability of Option.pdf:/Users/abel/Zotero/storage/585RVFSH/Lim - Naïve Bayes Classifier And Profitability of Option.pdf:application/pdf}
}

@article{knoblauch_doubly_nodate,
	title = {Doubly {Robust} {Bayesian} {Inference} for {Non}-{Stationary} {Streaming} {Data} with β-{Divergences}},
	abstract = {We present the ﬁrst robust Bayesian Online Changepoint Detection algorithm through General Bayesian Inference (GBI) with β-divergences. The resulting inference procedure is doubly robust for both the parameter and the changepoint (CP) posterior, with linear time and constant space complexity. We provide a construction for exponential models and demonstrate it on the Bayesian Linear Regression model. In so doing, we make two additional contributions: Firstly, we make GBI scalable using Structural Variational approximations that are exact as β → 0. Secondly, we give a principled way of choosing the divergence parameter β by minimizing expected predictive loss on-line. Reducing False Discovery Rates of CPS from over 90\% to 0\% on real world data, this offers the state of the art.},
	language = {en},
	author = {Knoblauch, Jeremias and Jewson, Jack and Damoulas, Theodoros},
	pages = {12},
	file = {Knoblauch et al. - Doubly Robust Bayesian Inference for Non-Stationar.pdf:/Users/abel/Zotero/storage/SFQ3ZBH5/Knoblauch et al. - Doubly Robust Bayesian Inference for Non-Stationar.pdf:application/pdf}
}

@article{guo_explaining_nodate,
	title = {Explaining {Deep} {Learning} {Models} -- {A} {Bayesian} {Non}-parametric {Approach}},
	abstract = {Understanding and interpreting how machine learning (ML) models make decisions have been a big challenge. While recent research has proposed various technical approaches to provide some clues as to how an ML model makes individual predictions, they cannot provide users with an ability to inspect a model as a complete entity. In this work, we propose a novel technical approach that augments a Bayesian non-parametric regression mixture model with multiple elastic nets. Using the enhanced mixture model, we can extract generalizable insights for a target model through a global approximation. To demonstrate the utility of our approach, we evaluate it on different ML models in the context of image recognition. The empirical results indicate that our proposed approach not only outperforms the state-of-the-art techniques in explaining individual decisions but also provides users with an ability to discover the vulnerabilities of the target ML models.},
	language = {en},
	author = {Guo, Wenbo and Huang, Sui and Tao, Yunzhe and Xing, Xinyu and Lin, Lin},
	pages = {11},
	file = {Guo et al. - Explaining Deep Learning Models -- A Bayesian Non-.pdf:/Users/abel/Zotero/storage/BXVFZIJV/Guo et al. - Explaining Deep Learning Models -- A Bayesian Non-.pdf:application/pdf}
}

@article{shah_bayesian_nodate,
	title = {Bayesian {Inference} of {Temporal} {Task} {Specifications} from {Demonstrations}},
	abstract = {When observing task demonstrations, human apprentices are able to identify whether a given task is executed correctly long before they gain expertise in actually performing that task. Prior research into learning from demonstrations (LfD) has failed to capture this notion of the acceptability of an execution; meanwhile, temporal logics provide a ﬂexible language for expressing task speciﬁcations. Inspired by this, we present Bayesian speciﬁcation inference, a probabilistic model for inferring task speciﬁcation as a temporal logic formula. We incorporate methods from probabilistic programming to deﬁne our priors, along with a domain-independent likelihood function to enable sampling-based inference. We demonstrate the efﬁcacy of our model for inferring speciﬁcations with over 90\% similarity between the inferred speciﬁcation and the ground truth, both within a synthetic domain and a real-world table setting task.},
	language = {en},
	author = {Shah, Ankit and Kamath, Pritish and Shah, Julie A and Li, Shen},
	pages = {10},
	file = {Shah et al. - Bayesian Inference of Temporal Task Specifications.pdf:/Users/abel/Zotero/storage/BZ7IUDEE/Shah et al. - Bayesian Inference of Temporal Task Specifications.pdf:application/pdf}
}

@article{ng_bayesian_nodate,
	title = {Bayesian {Semi}-supervised {Learning} with {Graph} {Gaussian} {Processes}},
	abstract = {We propose a data-efﬁcient Gaussian process-based Bayesian approach to the semisupervised learning problem on graphs. The proposed model shows extremely competitive performance when compared to the state-of-the-art graph neural networks on semi-supervised learning benchmark experiments, and outperforms the neural networks in active learning experiments where labels are scarce. Furthermore, the model does not require a validation data set for early stopping to control over-ﬁtting. Our model can be viewed as an instance of empirical distribution regression weighted locally by network connectivity. We further motivate the intuitive construction of the model with a Bayesian linear model interpretation where the node features are ﬁltered by an operator related to the graph Laplacian. The method can be easily implemented by adapting off-the-shelf scalable variational inference algorithms for Gaussian processes.},
	language = {en},
	author = {Ng, Yin Cheng and Colombo, Nicolò and Silva, Ricardo},
	pages = {12},
	file = {Ng et al. - Bayesian Semi-supervised Learning with Graph Gauss.pdf:/Users/abel/Zotero/storage/FCTPWY4Y/Ng et al. - Bayesian Semi-supervised Learning with Graph Gauss.pdf:application/pdf}
}

@article{michoel_analytic_nodate,
	title = {Analytic solution and stationary phase approximation for the {Bayesian} lasso and elastic net},
	abstract = {The lasso and elastic net linear regression models impose a double-exponential prior distribution on the model parameters to achieve regression shrinkage and variable selection, allowing the inference of robust models from large data sets. However, there has been limited success in deriving estimates for the full posterior distribution of regression coefﬁcients in these models, due to a need to evaluate analytically intractable partition function integrals. Here, the Fourier transform is used to express these integrals as complex-valued oscillatory integrals over “regression frequencies”. This results in an analytic expansion and stationary phase approximation for the partition functions of the Bayesian lasso and elastic net, where the non-differentiability of the double-exponential prior has so far eluded such an approach. Use of this approximation leads to highly accurate numerical estimates for the expectation values and marginal posterior distributions of the regression coefﬁcients, and allows for Bayesian inference of much higher dimensional models than previously possible.},
	language = {en},
	author = {Michoel, Tom},
	pages = {11},
	file = {Michoel - Analytic solution and stationary phase approximati.pdf:/Users/abel/Zotero/storage/BCUGH9AC/Michoel - Analytic solution and stationary phase approximati.pdf:application/pdf}
}

@article{diffey_new_2017,
	title = {A new {REML} (parameter expanded) {EM} algorithm for linear mixed models},
	volume = {59},
	issn = {1467-842X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/anzs.12208},
	doi = {10.1111/anzs.12208},
	abstract = {Linear mixed models are regularly applied to animal and plant breeding data to evaluate genetic potential. Residual maximum likelihood (REML) is the preferred method for estimating variance parameters associated with this type of model. Typically an iterative algorithm is required for the estimation of variance parameters. Two algorithms which can be used for this purpose are the expectation-maximisation (EM) algorithm and the parameter expanded EM (PX-EM) algorithm. Both, particularly the EM algorithm, can be slow to converge when compared to a Newton-Raphson type scheme such as the average information (AI) algorithm. The EM and PX-EM algorithms require specification of the complete data, including the incomplete and missing data. We consider a new incomplete data specification based on a conditional derivation of REML. We illustrate the use of the resulting new algorithm through two examples: a sire model for lamb weight data and a balanced incomplete block soybean variety trial. In the cases where the AI algorithm failed, a REML PX-EM based on the new incomplete data specification converged in 28\% to 30\% fewer iterations than the alternative REML PX-EM specification. For the soybean example a REML EM algorithm using the new specification converged in fewer iterations than the current standard specification of a REML PX-EM algorithm. The new specification integrates linear mixed models, Henderson's mixed model equations, REML and the REML EM algorithm into a cohesive framework.},
	language = {en},
	number = {4},
	urldate = {2019-04-02},
	journal = {Australian \& New Zealand Journal of Statistics},
	author = {Diffey, S. M. and Smith, A. B. and Welsh, A. H. and Cullis, B. R.},
	year = {2017},
	keywords = {mixed model equations, variance components},
	pages = {433--448},
	file = {Full Text PDF:/Users/abel/Zotero/storage/QPPIMB9U/Diffey et al. - 2017 - A new REML (parameter expanded) EM algorithm for l.pdf:application/pdf;Snapshot:/Users/abel/Zotero/storage/ZJ8RSVUM/anzs.html:text/html}
}

@article{meiring_optimal_2018,
	title = {Optimal intensive care outcome prediction over time using machine learning},
	volume = {13},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0206862},
	doi = {10.1371/journal.pone.0206862},
	abstract = {Background Prognostication is an essential tool for risk adjustment and decision making in the intensive care unit (ICU). Research into prognostication in ICU has so far been limited to data from admission or the first 24 hours. Most ICU admissions last longer than this, decisions are made throughout an admission, and some admissions are explicitly intended as time-limited prognostic trials. Despite this, temporal changes in prognostic ability during ICU admission has received little attention to date. Current predictive models, in the form of prognostic clinical tools, are typically derived from linear models and do not explicitly handle incremental information from trends. Machine learning (ML) allows predictive models to be developed which use non-linear predictors and complex interactions between variables, thus allowing incorporation of trends in measured variables over time; this has made it possible to investigate prognosis throughout an admission. Methods and findings This study uses ML to assess the predictability of ICU mortality as a function of time. Logistic regression against physiological data alone outperformed APACHE-II and demonstrated several important interactions including between lactate \& noradrenaline dose, between lactate \& MAP, and between age \& MAP consistent with the current sepsis definitions. ML models consistently outperformed logistic regression with Deep Learning giving the best results. Predictive power was maximal on the second day and was further improved by incorporating trend data. Using a limited range of physiological and demographic variables, the best machine learning model on the first day showed an area under the receiver-operator characteristic curve (AUC) of 0.883 (σ = 0.008), compared to 0.846 (σ = 0.010) for a logistic regression from the same predictors and 0.836 (σ = 0.007) for a logistic regression based on the APACHE-II score. Adding information gathered on the second day of admission improved the maximum AUC to 0.895 (σ = 0.008). Beyond the second day, predictive ability declined. Conclusion This has implications for decision making in intensive care and provides a justification for time-limited trials of ICU therapy; the assessment of prognosis over more than one day may be a valuable strategy as new information on the second day helps to differentiate outcomes. New ML models based on trend data beyond the first day could greatly improve upon current risk stratification tools.},
	language = {en},
	number = {11},
	urldate = {2019-04-02},
	journal = {PLOS ONE},
	author = {Meiring, Christopher and Dixit, Abhishek and Harris, Steve and MacCallum, Niall S. and Brealey, David A. and Watkinson, Peter J. and Jones, Andrew and Ashworth, Simon and Beale, Richard and Brett, Stephen J. and Singer, Mervyn and Ercole, Ari},
	month = nov,
	year = {2018},
	keywords = {Decision making, Deep learning, Forecasting, Intensive care units, Machine learning, Machine learning algorithms, Neural networks, Sepsis},
	pages = {e0206862},
	file = {Full Text PDF:/Users/abel/Zotero/storage/RFVY6ERA/Meiring et al. - 2018 - Optimal intensive care outcome prediction over tim.pdf:application/pdf;Snapshot:/Users/abel/Zotero/storage/RFIA2IWJ/article.html:text/html}
}

@article{dempster_maximum_1977,
	title = {Maximum {Likelihood} from {Incomplete} {Data} via the {EM} {Algorithm}},
	volume = {39},
	url = {http://www.jstor.org/stable/2984875},
	abstract = {A broadlyapplicablealgorithmforcomputinmg aximumlikelihoodestimatefsrom incompletde ata is presentedat variouslevelsof generalityT. heoryshowingthe monotonebehaviourof thelikelihoodand convergencoefthealgorithmis derived. Manyexamplesare sketchedi,ncludingmissingvalue situationsa,pplicationsto grouped,censoredor truncateddata, finitemixturemodels,variancecomponent estimationh, yperparameterstimationi,terativelyreweightedleast squares and factoranalysis.},
	language = {en},
	number = {1},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Dempster, A. P. and Laird, N. M. and work(s):, D. B. Rubin Reviewed},
	year = {1977},
	pages = {1--38},
	file = {Dempster et al. - 1977 - Maximum Likelihood from Incomplete Data via the EM.pdf:/Users/abel/Zotero/storage/B55AS8JY/Dempster et al. - 1977 - Maximum Likelihood from Incomplete Data via the EM.pdf:application/pdf}
}