\documentclass[12pt,letterpaper,fleqn]{hmcpset}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{parskip}

\input{macros.tex}

% info for header block in upper right hand corner
\name{Abel Sapirstein}
\class{Math189R SP19}
\assignment{Homework 6}
\duedate{Monday, Apr 1, 2019}

\begin{document}

Feel free to work with other students, but make sure you write up the homework
and code on your own (no copying homework \textit{or} code; no pair programming).
Feel free to ask students or instructors for help debugging code or whatever else,
though.
\newline
\newline
The starter files for problem 2 can be found under the Resource tab on course website. Please print out all the graphs generated by your own code and submit them together with the written part, and make sure you upload the code to your Github repository.\\

\begin{problem}[1]
\textbf{(Murphy 11.2 - EM for Mixtures of Gaussians)} Show that the M step for ML
estimation of a mixture of Gaussians is given by
\begin{align*}
    \mub_k &= \frac{\sum_i r_{ik}\xx_i}{r_k}\\
    \Sigmab_k &= \frac{1}{r_k}\sum_i r_{ik}(\xx_i - \mub_k)(\xx_i - \mub_k)^\T = \frac{1}{r_k}\sum_i r_{ik}\xx_i\xx_i^\T - r_k\mub_k\mub_k^\T.
\end{align*}
\end{problem}
\begin{solution}
Murphy states the the log likelihood for mixed Gaussians is; 
$$ \ell (\mub_K, \Sigmab_k) = -\frac{1}{2}\sum_i r_{ik} \left[ \log |\Sigmab_k| + (x_i-\mub_k)^\T \sigmab_k^{-1}(x_i-\mub_k)\right]$$
\begin{enumerate}
\item[$\mub_k.$] Lets maximize this by setting $\frac{d\ell}{d\mub_k} = 0 $ and solving for $\mub_k$
$$ \frac{d\ell}{d\mub_k}=\frac{d}{d\mub_k}\left(-\frac{1}{2}\sum_i r_{ik} \left[ \log |\Sigmab_k| + (x_i-\mub_k)^\T \sigmab_k^{-1}(x_i-\mub_k)\right]\right) =  -\frac{1}{2}\sum_i r_{ik} \Sigmab_k^{-1}(x_i-\mub_k)$$ 
$$\sum_i r_{ik} \mub_k = \sum_i r_{ik} x_i$$
$$\mub_k =  \frac{\sum_i r_{ik} x_i}{r_k} $$
\item[$\Sigmab_k$] Again using the same method; 
$$\frac{d\ell}{d \Sigmab_k} = -\frac{1}{2} \sigma r_{ik} \left[ \Sigma^{-1} - \Sigma^{-1} (x_i - \mub_k) ^\T (x_i- \mub_k)\Sigmab_k^{-1}\right]= 0$$
We can simplify this to be 

$$\sum_i r_{ik} \textbf{I} = \Sigma_k^{-1} \Sigma _i r_{ik} \left[(x_i - \mub_k)(x_i - \mub_k) ^\T \right]$$
This is equivalent to $$ \Sigmab_k = \frac{1}{r_k}\sum_i r_{ik}(\xx_i - \mub_k)(\xx_i - \mub_k)^\T = \frac{1}{r_k}\sum_i r_{ik}\xx_i\xx_i^\T - r_k\mub_k\mub_k^\T.$$
\end{enumerate}
\end{solution}
\newpage



\begin{problem}[2]
\textbf{(SVD Image Compression)}
In this problem, we will use the image of a scary clown online to perform image compression.  In the starter code, we have already load the image into a matrix/array for you. However, you might need internet connection to access the image and therefore successfully run the starter code. The code requires Python library Pillow in order to run.
\newline
\newline 
Plot the progression of the 100 largest singular values for the original image
and a randomly shuffled version of the same image (all on the same plot). In a single figure plot
a grid of four images: the original image, and a rank $k$ truncated SVD approximation of the original
image for $k\in\{2,10,20\}$.

\end{problem}
\begin{solution}
\vfill
\end{solution}
\newpage

\end{document}
